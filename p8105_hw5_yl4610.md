p8105_hw5_yl4610
================
2022-11-08

``` r
library(tidyverse)
```

# problem 1

# problem 2

``` r
homicide_raw = read_csv( "./data/homicide_data.csv") 
```

This dataset contains variables such as uid, reported_date, victim_last,
victim_first, victim_race, victim_age, victim_sex, city, state, lat,
lon, disposition. It contains 12 columns and 52179 rows.

``` r
homicide = 
homicide_raw %>% 
  mutate (
    city_state = str_c(city, ", ", state),
    city_state = ifelse(city_state == "Tulsa, AL", "Tulsa, OK", city_state)) %>%
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition !="Closed by arrest"))

homicide
```

    ## # A tibble: 50 × 3
    ##    city_state      total_homicides unsolved_homicides
    ##    <chr>                     <int>              <int>
    ##  1 Albuquerque, NM             378                146
    ##  2 Atlanta, GA                 973                373
    ##  3 Baltimore, MD              2827               1825
    ##  4 Baton Rouge, LA             424                196
    ##  5 Birmingham, AL              800                347
    ##  6 Boston, MA                  614                310
    ##  7 Buffalo, NY                 521                319
    ##  8 Charlotte, NC               687                206
    ##  9 Chicago, IL                5535               4073
    ## 10 Cincinnati, OH              694                309
    ## # … with 40 more rows

I used mutate to create a city_state variable. Then I used group_by and
summarize to summarize within cities to obtain the total number of
homicides and the number of unsolved homicides.

``` r
baltimore = 
  homicide %>% 
  filter(city_state == "Baltimore, MD") 

prop.test(
  baltimore$unsolved_homicides,baltimore$total_homicides) %>% 
  broom::tidy()
```

    ## # A tibble: 1 × 8
    ##   estimate statistic  p.value parameter conf.low conf.high method        alter…¹
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>         <chr>  
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample pro… two.si…
    ## # … with abbreviated variable name ¹​alternative

I used filter to filter out other city_state except “Baltimore, MD”.
Then I used prop.test function to estimate the proportion of homicides
that are unsolved and applied the broom::tidy to pull the estimated
proportion and confidence intervals.

``` r
city_final = 
  homicide %>% 
  mutate(
    outputs_df = map2 (unsolved_homicides,total_homicides, prop.test),
    final_outputs_df = map(outputs_df, broom::tidy)) %>% 
  unnest(final_outputs_df) %>% 
  select (city_state, estimate, conf.low, conf.high)

city_final
```

    ## # A tibble: 50 × 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483
    ## # … with 40 more rows

I used mutate to run prop.test for each of the cities and extract both
the proportion of unsolved homicides and the confidence interval for
each via purrr::map and purrr::map2. Then I unnested the
final_outputs_df and selected variables I wanted to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
city_plot = 
city_final %>% 
ggplot(aes(x=fct_reorder(city_state, estimate), y=estimate))+
  geom_point()+
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high))+
  labs(
    title = "Estimates and confidence intervals of unsolved homocide for each city",
    x = "cities") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

city_plot
```

![](p8105_hw5_yl4610_files/figure-gfm/unnamed-chunk-5-1.png)<!-- --> I
used ggplot to create a scatter plot showing the estimates and CIs for
each city. I used geom_errorbar to add error bars based on the upper and
lower limits. I used fct_reorder to organize cities according to the
proportion of unsolved homicides. lastly, I used labs to add title and
x-axis label.

# problem 3

``` r
norm_dis = function(n = 30, mu, sigma = 5) {
  norm_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)) 
  
    norm_data %>% 
    t.test (mu = 0, alpha = 0.05, conf.level = 0.95) %>% 
      broom::tidy ()
}
norm_dis
```

    ## function(n = 30, mu, sigma = 5) {
    ##   norm_data = tibble(
    ##     x = rnorm(n, mean = mu, sd = sigma)) 
    ##   
    ##     norm_data %>% 
    ##     t.test (mu = 0, alpha = 0.05, conf.level = 0.95) %>% 
    ##       broom::tidy ()
    ## }

``` r
norm_results_df = 
  expand_grid(
    mu = 0:6,
    iter = 1:10) %>% 
  mutate(
    estimate_df = map(mu, ~norm_dis(mu=.x))
  ) %>% 
  unnest(estimate_df) 

norm_results_df
```

    ## # A tibble: 70 × 10
    ##       mu  iter estimate statistic p.value param…¹ conf.…² conf.…³ method alter…⁴
    ##    <int> <int>    <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>  <chr>  
    ##  1     0     1    0.211     0.231   0.819      29  -1.66    2.08  One S… two.si…
    ##  2     0     2    0.349     0.304   0.763      29  -2.00    2.69  One S… two.si…
    ##  3     0     3   -0.520    -0.482   0.633      29  -2.73    1.69  One S… two.si…
    ##  4     0     4   -1.04     -1.09    0.285      29  -2.98    0.910 One S… two.si…
    ##  5     0     5    0.795     0.755   0.456      29  -1.36    2.95  One S… two.si…
    ##  6     0     6   -0.777    -0.792   0.435      29  -2.78    1.23  One S… two.si…
    ##  7     0     7    0.965     1.18    0.246      29  -0.701   2.63  One S… two.si…
    ##  8     0     8    0.145     0.143   0.887      29  -1.92    2.21  One S… two.si…
    ##  9     0     9    0.194     0.201   0.842      29  -1.79    2.17  One S… two.si…
    ## 10     0    10   -0.262    -0.255   0.801      29  -2.37    1.84  One S… two.si…
    ## # … with 60 more rows, and abbreviated variable names ¹​parameter, ²​conf.low,
    ## #   ³​conf.high, ⁴​alternative

I conducted a simulation in a one-sample t-test with n=30,
mu=0,1,2,3,4,5,6, and sd=5 via function. I generated 5000 datasets. \#
change ite size to 5000 at the end

``` r
plot_prop1 = 
norm_results_df %>%
  group_by(mu) %>%
  summarize(
    sum_pvalue = sum(p.value < 0.05),
    sum_n = n()) %>% 
  mutate (y_value = sum_pvalue/sum_n) %>% 
  ggplot(aes(x = mu, y = y_value)) + 
  geom_point() + 
  labs(title = "Proportion of times the null was rejected vs True means", 
       x = "True means",
       y = "Proportion of times the null was rejected") 

plot_prop1
```

![](p8105_hw5_yl4610_files/figure-gfm/unnamed-chunk-8-1.png)<!-- --> I
used ggplot to create a scatter plot showing the proportion of times the
null was rejected (the power of the test) on the y axis and the true
value of μ on the x axis. As true mean increases, proportion of times
the null was rejected increases. \# effect sizes? correct calculation?

``` r
plot_prop2 = 
norm_results_df %>%
  group_by(mu) %>%
  summarize(ave_estimate_mu = mean(estimate)) %>%
  ggplot(aes(x = mu, y = ave_estimate_mu, group = mu)) + 
  geom_point() + 
  labs(title = "Average estimate of mu hat vs True value of mu", 
       x = "True value of mu", 
       y = "Average estimate of mu hat") 
plot_prop2
```

![](p8105_hw5_yl4610_files/figure-gfm/unnamed-chunk-9-1.png)<!-- --> I
used ggplot to make a plot showing the average estimate of mu hat on the
y axis and the true value of mu on the x axis.

``` r
plot_prop3 = 
norm_results_df %>%
  group_by(mu) %>%
  filter(p.value < 0.05) %>% 
  summarize(ave_estimate_mu = mean(estimate)) %>%
  ggplot(aes(x = mu, y = ave_estimate_mu, group = mu)) + 
  geom_point() + 
  labs(title = "Average estimate of mu hat vs True value of mu", 
       x = "True value of mu", 
       y = "Average estimate of mu hat only in samples for which the null was rejected") 
plot_prop3
```

![](p8105_hw5_yl4610_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

I used ggplot to make a second plot showing the average estimate of mu
hat only in samples for which the null was rejected on the y axis and
the true value of mu on the x axis.

The sample average of mu hat across tests for which the null is rejected
is approximately equal to the true value of mu when mu equals to 5 and 6
because as power and effect sizes increase, XXX
